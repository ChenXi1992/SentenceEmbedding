{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file & Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  \n",
    "import numpy as np \n",
    "import os \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import Model\n",
    "import operator\n",
    "\n",
    "import preTool\n",
    "\n",
    "embeddingPath = \"Embedding/\"\n",
    "gloveFile = 'glove.6B.300d.txt'  \n",
    "wordDim = 300 \n",
    "\n",
    "textPath = \"text/\"\n",
    "orgFile = \"test.txt\"\n",
    "\n",
    "processedPath = \"processed/\"\n",
    "maskedFile = \"masked.txt\"\n",
    "marked_token = \"maskedtoken\"\n",
    "\n",
    "frequentVocabLimit = 2500\n",
    "trainingSize = 0\n",
    "maxLength = 30\n",
    "vocab_size = 0\n",
    "\n",
    "val_pert = 0.0001\n",
    "\n",
    "wordDicFile = 'wordDic.txt'\n",
    "tokenizerFile = 'tokenizer.txt'\n",
    "embeddingFile =  'embedding_matrix.txt'\n",
    "valiFile = \"val_set.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Once and get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Splite the data into 10 groups  + Validatio dataset\n",
    "orgSentence = preTool.loadFile(textPath,orgFile)\n",
    "val_pert = 0.01\n",
    "val_len = int(len(orgSentence)*val_pert)\n",
    "val_set = orgSentence[-val_len:]\n",
    "\n",
    "# Preprocessing val_set and save to directory. \n",
    "orgSentence = orgSentence[:-val_len]\n",
    "orgLength = len(orgSentence)\n",
    "\n",
    "val_set  = preTool.preprocessing(val_set)\n",
    "\n",
    "with open(processedPath + textPath + valiFile, 'w') as f:\n",
    "    for item in val_set:\n",
    "        f.write(\"%s\" % item)\n",
    "print(\"Finish writing\")\n",
    "\n",
    "del val_set\n",
    "del orgSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 10 groups\n",
    "cut_pert = int(orgLength*0.1)\n",
    "split_group = 10\n",
    "\n",
    "for i in range(split_group):\n",
    "    orgSentence = preTool.loadFile(textPath,orgFile)\n",
    "    orgSentence = orgSentence[:-val_len]\n",
    "    orgSentence = orgSentence[i*cut_pert:(i+1)*cut_pert]\n",
    "    print(\"Start at {}, end at {}\".format(i*cut_pert,min((i+1)*cut_pert,orgLength)))\n",
    "    \n",
    "    orgSentence = preTool.preprocessing(orgSentence)\n",
    "    with open(processedPath + textPath + \"group_\"+str(i)+\".txt\", 'w') as f:\n",
    "        for item in orgSentence:\n",
    "            f.write(\"%s\" % item)\n",
    "    print(\"Finish writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orgFile = \"books_large_p2.txt\"\n",
    "for i in range(split_group):\n",
    "    orgSentence = loadFile(textPath,orgFile)\n",
    "    sentenceLen = len(orgSentence)\n",
    "    orgSentence = orgSentence[i*cut_pert:(i+1)*cut_pert]\n",
    "    print(\"Start at {}, end at {}\".format(i*cut_pert,min((i+1)*cut_pert,sentenceLen)))\n",
    "    \n",
    "    orgSentence = preprocessing(orgSentence)\n",
    "    with open(processedPath + textPath + \"group_\"+str( i + 10)+\".txt\", 'w') as f:\n",
    "        for item in orgSentence:\n",
    "            f.write(\"%s\" % item)\n",
    "    print(\"Finish writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the vocab frequency & tokenizer & embedding_matrix\n",
    "wordDic = {}\n",
    "for file in os.listdir(processedPath+textPath):\n",
    "    if file.endswith(\".txt\"):\n",
    "        lines = preTool.loadFile(processedPath+textPath , file)\n",
    "        lines = preTool.sentenceToWordList(lines)\n",
    "        wordDic = preTool.getVocabFrequencyForText(lines,wordDic)\n",
    "wordDic[marked_token] = 1 \n",
    "\n",
    "with open(processedPath +  wordDicFile, \"wb\") as internal_filename:\n",
    "    pickle.dump(wordDic, internal_filename)\n",
    "    \n",
    "    \n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(wordDic.keys())\n",
    "with open(processedPath +  tokenizerFile, \"wb\") as internal_filename:\n",
    "    pickle.dump(t, internal_filename)\n",
    "    \n",
    "vocab_size = len(t.word_index) + 2\n",
    "\n",
    "embeddings_matrix = preTool.getEmbeddingMatrix(embeddingPath,gloveFile,marked_token,wordDim,vocab_size)\n",
    "\n",
    "with open(processedPath +  embeddingFile, \"wb\") as internal_filename:\n",
    "    pickle.dump(embedding_matrix, internal_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dic & EmbeddingMatrix & Tokenizer & validation dataset  outside of the for-loop \n",
    "\n",
    "import copy\n",
    "dic = {}\n",
    "\n",
    "with open(processedPath + wordDicFile , \"rb\") as input_file:\n",
    "    wholeVocab = pickle.load(input_file)\n",
    "wholeVocab = sorted(wholeVocab.items(), key=operator.itemgetter(1))\n",
    "wholeVocab = wholeVocab[-frequentVocabLimit:]\n",
    "for key in wholeVocab:\n",
    "    dic[key[0]] = True\n",
    "\n",
    "with open(processedPath + tokenizerFile , \"rb\") as input_file:\n",
    "    t = pickle.load(input_file)\n",
    "    \n",
    "with open(processedPath + embeddingFile , \"rb\") as input_file:\n",
    "    embedding_matrix = pickle.load(input_file)\n",
    "\n",
    "orgFile,maskedFile = sentenceToTokenData(processedPath + textPath,valiFile,t,dic)\n",
    "input_X_vali,input_masked_first_vali,input_masked_second_vali,input_masked_third_vali,input_Y_vali = generateTrainingDataSet(orgFile,maskedFile)\n",
    "\n",
    "del orgFile,maskedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "num_neurons = [wordDim*2,wordDim*2]\n",
    "kernel_reg = 0.001\n",
    "batch_size = 400\n",
    "epochs = 1\n",
    "\n",
    "count = 0\n",
    "merge_mode = 'concat'\n",
    "extractLayer = ['X_input_second','X_input_mask_2']\n",
    "checkPoint = checkPointPath+ \"weights-loss-{loss:.4f}-cata_acc-{categorical_accuracy:.4f}-val_loss-{val_loss:.4f}-cate_acc_val-{val_categorical_accuracy:.4f}.hdf5\"\n",
    "restoreCheckpoint = False\n",
    "vocab_size = len(t.word_index) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model.buildModel(input_X_vali,kernel_reg,num_neurons,merge_mode,vocab_size,maxLength,wordDim,embedding_matrix)\n",
    "Adam = optimizers.Adam(lr=5e-4)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=Adam, metrics=['categorical_accuracy'])\n",
    "model.save_weights(\"random.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    for file in os.listdir(processedPath+textPath):\n",
    "        if file.endswith(\".txt\") and file != \"val_set.txt\":\n",
    "            print(\"\\n*******************  Epoch:{}, Training on file:{}  ***********************************************\\n\".format(i+1,file))\n",
    "            orgFile,maskedFile = sentenceToTokenData(processedPath + textPath,file,t,dic)\n",
    "            input_X_train,input_masked_first_train,input_masked_second_train,input_masked_third_train,input_Y_train = generateTrainingDataSet(orgFile,maskedFile)\n",
    "            \n",
    "            if restoreCheckpoint:\n",
    "                list_of_files = glob.glob( checkPointPath + '*.hdf5') # * means all if need specific format then *.csv\n",
    "                if len(list_of_files) != 0:\n",
    "                    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "                    print(\"Load model:\" + latest_file)\n",
    "                    model.load_weights(latest_file)\n",
    "                else:\n",
    "                    print(\"No pre-trained model to load\")\n",
    "                restoreCheckpoint = False\n",
    "            checkpoint =  ModelCheckpoint(checkpointPath, monitor=('val_loss'), verbose=0, \n",
    "                                  save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "            model.fit([input_X_train,input_masked_first_train,input_masked_second_train,input_masked_third_train],input_Y_train,\n",
    "                      validation_data=([input_X_vali,input_masked_first_vali,input_masked_second_vali,input_masked_third_vali],input_Y_vali),\n",
    "                      shuffle=True,epochs=epochs,batch_size=batch_size,callbacks=[checkpoint])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = Model.extractHiddenState(extractLayer,model,input_X_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output =  np.concatenate((output[0],output[1]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output =  Model.extractHiddenState(extractLayer,model,input_X)\n",
    "results1  =  np.concatenate((output[0],output[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the hidden layer for SentEval \n",
    "x =  np.zeros((10,maxLength,wordDim))\n",
    "model = buildModel(x,kernel_reg,num_neurons,merge_mode)\n",
    "model.load_weights(checkpoint)\n",
    "output = extractHiddenState(layerName=extractLayer,model=model, predict_input=input_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Warning: Normally its '  but in the text is ’\n",
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"wo n\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"ca n\\’t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"wont\",\"will not\", phrase)\n",
    "    phrase = re.sub(r\"cant\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"wouldnt\", \"would not\", phrase)\n",
    "    phrase = re.sub(r\"couldnt\",\"could not\",phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def generateMaskedSentence(processedSentence,frequentDic):\n",
    "    print(\"Generate Maksed Sentence\")\n",
    "    for sentence in processedSentence:\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] not in frequentDic.keys():\n",
    "                sentence[i] = marked_token\n",
    "    return processedSentence\n",
    "\n",
    "def generateIndexForDataset(dataSize):\n",
    "    print(\"Generate Index For Dataset\")\n",
    "    trainIndexList = []\n",
    "    data_index = []\n",
    "\n",
    "    for i in range(dataSize-1):\n",
    "        x = randint(0,dataSize-1)\n",
    "        y = randint(0,dataSize-1)\n",
    "        z = i+1 \n",
    "        while(x == y or x==i or x==z or y == i or y==z ):\n",
    "            x = randint(0,dataSize-1)\n",
    "            y = randint(0,dataSize-1)\n",
    "        index = ([[x,0],[y,0],[z,1]])\n",
    "        random.shuffle(index)\n",
    "        trainIndexList.append([i,index])   \n",
    "\n",
    "    for row in trainIndexList:\n",
    "        train_x = row[0]\n",
    "        y_index = row[1]\n",
    "        y_output = []\n",
    "        x_masked = []\n",
    "        for index in y_index:\n",
    "            y_output.append(index[1])\n",
    "            x_masked.append(index[0])\n",
    "        data_index.append([train_x,x_masked,y_output])\n",
    "\n",
    "    print(\"Finish Generating index\")\n",
    "    return data_index\n",
    "\n",
    "def loadFile(textPath,orgFile):\n",
    "    print(\"Load:\" + textPath + orgFile)\n",
    "    with open(textPath +  orgFile ,encoding=\"utf-8\") as fp:\n",
    "        lines = fp.readlines()\n",
    "    lines = [e for e in lines if e not in {'\\n'}]\n",
    "    return lines\n",
    "    \n",
    "def preprocessing(lines): \n",
    "    print(\"Pre processing Data\")\n",
    "    count = 0 \n",
    "    for line in lines:\n",
    "        # Decapitalize: Conver to lower case first. \n",
    "        line = line.lower()\n",
    "        # Delete url \n",
    "        line = re.sub(r\"http\\S+\", \"link\", line)\n",
    "        line = re.sub(r\"\\S+html\", \"link\", line)\n",
    "        line = re.sub(r\"\\S+.com$\", \"link\", line)\n",
    "        line = re.sub(r\"\\S+.jpg$\", \"photo\", line)\n",
    "        line = decontracted(line)\n",
    "        '''\n",
    "        ignore:  * { } \\  < > \n",
    "        Splite based on : ! . ,  &  # ' $ \n",
    "        line = re.findall(r\"[\\w']+|[().,:!?;'$&]\", line)\n",
    "        '''\n",
    "        lines[count] = line\n",
    "        count += 1 \n",
    "    return lines\n",
    "\n",
    "def getVocabFrequencyForText(text, dic):\n",
    "    for sent in text:\n",
    "        for vocab in sent:\n",
    "            if vocab not in dic.keys():\n",
    "                dic[vocab] = 1\n",
    "            else: dic[vocab] += 1\n",
    "    return dic\n",
    "\n",
    "def sentenceToWordList(lines):\n",
    "    print(\"Convert sentence to word list\")\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        lines[count] = re.findall(r\"[\\w']+|[().,:!?;'$&]\", line)\n",
    "        count += 1\n",
    "    return lines\n",
    "\n",
    "\n",
    "def sentenceToTokenData(textPath,orgFile,t,dic):\n",
    "    processedLine = loadFile(textPath,orgFile)\n",
    "    processedLine = sentenceToWordList(processedLine)\n",
    "    # print(\"Deep copy\")\n",
    "    maskedLine = copy.deepcopy(processedLine)\n",
    "    maskedLine =  generateMaskedSentence(maskedLine,dic)\n",
    "    print(\"--Test-- Process:{} ; masked:{}\".format(processedLine[1],maskedLine[1]))\n",
    "  \n",
    "    processedLine = t.texts_to_sequences(processedLine)\n",
    "    maskedLine = t.texts_to_sequences(maskedLine)\n",
    "\n",
    "    processedLine = pad_sequences(processedLine, maxlen=maxLength, padding='post')\n",
    "    maskedLine = pad_sequences(maskedLine, maxlen=maxLength, padding='post')\n",
    "    return processedLine,maskedLine \n",
    "\n",
    "def generateTrainingDataSet(processedLine,maskedLine):\n",
    "    dataIndex = generateIndexForDataset(len(processedLine))\n",
    "    # Modeling\n",
    "    input_X = []\n",
    "    input_masked_first = []\n",
    "    input_masked_second = []\n",
    "    input_masked_third = []\n",
    "    input_Y = []\n",
    "    for index in dataIndex:\n",
    "        input_X.append(processedLine[index[0]])\n",
    "        input_Y.append(index[2])\n",
    "        input_masked_first.append(maskedLine[index[1][0]])\n",
    "        input_masked_second.append(maskedLine[index[1][1]])\n",
    "        input_masked_third.append(maskedLine[index[1][2]])\n",
    "    input_X = asarray(input_X)\n",
    "    input_masked_first = asarray(input_masked_first)\n",
    "    input_masked_second = asarray(input_masked_second)\n",
    "    input_masked_third = asarray(input_masked_third)\n",
    "    input_Y = asarray(input_Y)\n",
    "    return input_X,input_masked_first,input_masked_second,input_masked_third,input_Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
