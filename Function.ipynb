{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Warning: Normally its '  but in the text is ’\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def sentence2Vec(dataset,model,wordDim,maxLength):\n",
    "    sentenceVec = []\n",
    "    for sentence in dataset:\n",
    "        wordVec = []\n",
    "        for vocab in sentence:\n",
    "            if vocab == marked_token:\n",
    "                wordVec.append(np.ones(wordDim))\n",
    "            else:\n",
    "                try:\n",
    "                    vec =  model[vocab]\n",
    "                    wordVec.append(vec)\n",
    "                except :\n",
    "                    wordVec.append(np.zeros(wordDim))\n",
    "        sentenceVec.append(wordVec)\n",
    "\n",
    "    sentenceVec = keras.preprocessing.sequence.pad_sequences(sentenceVec, maxlen=maxLength, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    return sentenceVec \n",
    "\n",
    "\n",
    "# High frequency vocab dictionary \n",
    "def getHighFrequencyVocab(frequentVocabLimit,mergeList):\n",
    "    fdist1 = FreqDist(mergeList)\n",
    "    frequentVocab = fdist1.most_common(frequentVocabLimit)\n",
    "    frequentDic = {}\n",
    "    for vocab in frequentVocab:\n",
    "        frequentDic[vocab[0]] = True\n",
    "    return frequentDic\n",
    "\n",
    "\n",
    "def generateMaskedSentence(processedSentence,frequentDic):\n",
    "    for sentence in processedSentence:\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] not in frequentDic.keys():\n",
    "                sentence[i] = marked_token\n",
    "    return processedSentence\n",
    "\n",
    "def generateIndexForDataset(dataSize):\n",
    "    trainIndexList = []\n",
    "    data_index = []\n",
    "\n",
    "    for i in range(dataSize-1):\n",
    "        x = randint(0,dataSize-1)\n",
    "        y = randint(0,dataSize-1)\n",
    "        z = i+1 \n",
    "        while(x == y or x==i or x==z or y == i or y==z ):\n",
    "            x = randint(0,dataSize-1)\n",
    "            y = randint(0,dataSize-1)\n",
    "        index = ([[x,0],[y,0],[z,1]])\n",
    "        random.shuffle(index)\n",
    "        trainIndexList.append([i,index])   \n",
    "\n",
    "    for row in trainIndexList:\n",
    "        train_x = row[0]\n",
    "        y_index = row[1]\n",
    "        y_output = []\n",
    "        x_masked = []\n",
    "        for index in y_index:\n",
    "            y_output.append(index[1])\n",
    "            x_masked.append(index[0])\n",
    "        data_index.append([train_x,x_masked,y_output])\n",
    "\n",
    "    random.shuffle(data_index)\n",
    "\n",
    "    return data_index\n",
    "\n",
    "\n",
    "def loadEmbeddingModel(embeddingPath,word2VecFile,gloveFile,convertGloveToWordVec):\n",
    "    word2vec_output_file = embeddingPath + word2VecFile\n",
    "    glove_input_file = embeddingPath + gloveFile\n",
    "\n",
    "    if convertGloveToWordVec:\n",
    "        glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "    # load the Stanford GloVe model\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def loadFile(textPath,orgFile):\n",
    "    with open(textPath +  orgFile ,encoding=\"utf-8\") as fp:\n",
    "        lines = fp.readlines()\n",
    "    # Remove the empty line \n",
    "    lines = [e for e in lines if e not in {'\\n'}]\n",
    "    \n",
    "    return lines\n",
    "    \n",
    "def preprocessing(lines): \n",
    "    newLine = []\n",
    "    for line in lines:\n",
    "        # Decapitalize: Conver to lower case first. \n",
    "        line = line.lower()\n",
    "        # Delete url \n",
    "        line = re.sub(r\"http\\S+\", \"link\", line)\n",
    "        line = re.sub(r\"\\S+html\", \"link\", line)\n",
    "        line = re.sub(r\"\\S+.com$\", \"link\", line)\n",
    "        line = re.sub(r\"\\S+.jpg$\", \"photo\", line)\n",
    "        line = decontracted(line)\n",
    "\n",
    "        '''\n",
    "        ignore:  * { } \\  < > \n",
    "        Splite based on : ! . ,  &  # ' $ \n",
    "        line = re.findall(r\"[\\w']+|[().,:!?;'$&]\", line)\n",
    "        '''\n",
    "\n",
    "        line = re.findall(r\"[\\w']+|[().,:!?;'$&]\", line)\n",
    "\n",
    "        newLine.append(line)\n",
    "    \n",
    "    mergeList = []\n",
    "    for i in newLine:\n",
    "        mergeList += i   \n",
    "    return newLine, mergeList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file & Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle  \n",
    "from nltk import FreqDist\n",
    "import re\n",
    "import random\n",
    "from random import randint\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np \n",
    "\n",
    "embeddingPath = \"Embedding/\"\n",
    "convertGloveToWordVec = False  # The first time use Glove need to convert to Word2Vec format\n",
    "gloveFile = 'glove.840B.300d.txt'  \n",
    "word2VecFile =  'word2vec.6B.50d.txt'\n",
    "wordDim = 50\n",
    "\n",
    "textPath = \"text/\"\n",
    "orgFile = \"all.txt\"\n",
    "processedFile = \"process.txt\"\n",
    "maskedFile = \"masked.txt\"\n",
    "\n",
    "frequentVocabLimit = 2000\n",
    "marked_token = \"MASKED_TOKEN\"\n",
    "trainingSize = 0\n",
    "maxLength = 50\n",
    "dataSize = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, remove the empty line\n",
    "sentenceList = loadFile(textPath,orgFile)\n",
    "# Remove unwanted feature, proprocessing, return wordList\n",
    "processedLine, vocabList = preprocessing(sentenceList)\n",
    "\n",
    "dataSize = len(processedLine)\n",
    "# Get the high frequency vocabulary \n",
    "frequenentVocabDic =  getHighFrequencyVocab(frequentVocabLimit,vocabList)\n",
    "\n",
    "# Save processed data\n",
    "with open(textPath +  processedFile, \"wb\") as internal_filename:\n",
    "    pickle.dump(processedLine, internal_filename)\n",
    "\n",
    "# Generate the masked dataset\n",
    "\n",
    "maskedLine =  generateMaskedSentence(processedLine,frequenentVocabDic)\n",
    "\n",
    "# Save masked data\n",
    "with open(textPath +  maskedFile, \"wb\") as internal_filename:\n",
    "    pickle.dump(maskedLine, internal_filename)\n",
    "    \n",
    "# Generate the index for the dataset \n",
    "dataIndex = generateIndexForDataset(dataSize)\n",
    "\n",
    "# Read the data \n",
    "with open(textPath + processedFile, \"rb\") as input_file:\n",
    "    processedLine = pickle.load(input_file)\n",
    "with open(textPath + maskedFile , \"rb\") as input_file:\n",
    "    maskedSentence = pickle.load(input_file)\n",
    "    \n",
    "model = loadEmbeddingModel(embeddingPath,word2VecFile,gloveFile,convertGloveToWordVec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceVec =  sentence2Vec(processedLine,model,wordDim,maxLength)\n",
    "maskedSentenceVec = sentence2Vec(processedLine,model,wordDim,maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate input_X, input_masked_X, input_Y\n",
    "input_X = []\n",
    "input_masked_first = []\n",
    "input_masked_second = []\n",
    "input_masked_third = []\n",
    "input_Y = []\n",
    "for index in dataIndex:\n",
    "    input_X.append(sentenceVec[index[0]])\n",
    "    input_Y.append(index[2])\n",
    "    input_masked_first.append(maskedSentenceVec[index[1][0]])\n",
    "    input_masked_second.append(maskedSentenceVec[index[1][1]])\n",
    "    input_masked_third.append(maskedSentenceVec[index[1][2]])\n",
    "    \n",
    "input_X = np.array(input_X)\n",
    "input_masked_first = np.array(input_masked_first)\n",
    "input_masked_second = np.array(input_masked_second)\n",
    "input_masked_third = np.array(input_masked_third)\n",
    "input_Y = np.array(input_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Modeling  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenxi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Bidirectional,Input, Dropout, Flatten, concatenate, dot, GaussianDropout, Activation, GRU\n",
    "from keras.regularizers import l1, l2\n",
    "import keras.backend as kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_neurons = [100,100]\n",
    "kernel_reg = 0.01\n",
    "merge_mode = 'concat'\n",
    "batch_size = 258\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(input_X,kernel_reg,num_neurons,merge_mode):\n",
    "    org_input = Input(shape=(input_X.shape[1],input_X.shape[2]), name = 'org_input')\n",
    "    org_input_1 = Bidirectional(GRU(num_neurons[0],return_sequences= True,kernel_regularizer=l2(kernel_reg)), merge_mode= merge_mode,name='X_input_first')(org_input)\n",
    "    org_input_2 = Bidirectional(GRU(num_neurons[1],kernel_regularizer=l2(kernel_reg)), merge_mode= merge_mode,name='X_input_second')(org_input_1)\n",
    "\n",
    "    # Create shared layer for masked sentence \n",
    "    shared_first = Bidirectional(GRU(num_neurons[0],return_sequences= True,kernel_regularizer=l2(kernel_reg)), merge_mode= merge_mode,name='X_input_mask_1')\n",
    "    shared_second = Bidirectional(GRU(num_neurons[1],kernel_regularizer=l2(kernel_reg)), merge_mode= merge_mode,name='X_input_mask_2')\n",
    "\n",
    "    mask_input_first = Input(shape=(input_X.shape[1],input_X.shape[2]), name = 'mask_input_first')\n",
    "    mask_input_first_1 = shared_first(mask_input_first)\n",
    "    mask_input_first_2 = shared_second(mask_input_first_1)\n",
    "\n",
    "    mask_input_second = Input(shape=(input_X.shape[1],input_X.shape[2]), name = 'mask_input_second')\n",
    "    mask_input_second_1 = shared_first(mask_input_second)\n",
    "    mask_input_second_2 = shared_second(mask_input_second_1)\n",
    "\n",
    "    mask_input_third = Input(shape=(input_X.shape[1],input_X.shape[2]), name = 'mask_input_third')\n",
    "    mask_input_third_1 = shared_first(mask_input_third)\n",
    "    mask_input_third_2 = shared_second(mask_input_third_1)\n",
    "\n",
    "    out1 = dot([org_input_2, mask_input_first_2], axes=1, name='output_1')\n",
    "    out2 = dot([org_input_2, mask_input_second_2], axes=1, name='output_2')\n",
    "    out3 = dot([org_input_2, mask_input_third_2], axes=1, name='output_3')\n",
    "\n",
    "    concat = concatenate([out1, out2, out3], name='concat')\n",
    "\n",
    "    output = Activation('softmax')(concat)\n",
    "\n",
    "    model = Model(inputs=[org_input,mask_input_first,mask_input_second,mask_input_third], outputs=output)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org_dim_1 dimension is:(None, 50, 200), org_dim_2 dimension is: (None, 200)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "org_input (InputLayer)          (None, 50, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_input_first (InputLayer)   (None, 50, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_input_second (InputLayer)  (None, 50, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_input_third (InputLayer)   (None, 50, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "X_input_first (Bidirectional)   (None, 50, 200)      90600       org_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "X_input_mask_1 (Bidirectional)  (None, 50, 200)      90600       mask_input_first[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "X_input_mask_second_1 (Bidirect (None, 50, 200)      90600       mask_input_second[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "X_input_mask_third_1 (Bidirecti (None, 50, 200)      90600       mask_input_third[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "X_input_second (Bidirectional)  (None, 200)          180600      X_input_first[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "X_input_mask_2 (Bidirectional)  (None, 200)          180600      X_input_mask_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "X_input_mask_second_2 (Bidirect (None, 200)          180600      X_input_mask_second_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "X_input_mask_third_2 (Bidirecti (None, 200)          180600      X_input_mask_third_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "output_1 (Dot)                  (None, 1)            0           X_input_second[0][0]             \n",
      "                                                                 X_input_mask_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output_2 (Dot)                  (None, 1)            0           X_input_second[0][0]             \n",
      "                                                                 X_input_mask_second_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "output_3 (Dot)                  (None, 1)            0           X_input_second[0][0]             \n",
      "                                                                 X_input_mask_third_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 3)            0           output_1[0][0]                   \n",
      "                                                                 output_2[0][0]                   \n",
      "                                                                 output_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 3)            0           concat[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,084,800\n",
      "Trainable params: 1,084,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = buildModel(input_X,kernel_reg,num_neurons,merge_mode)\n",
    "model.compile( loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "12793/12793 [==============================] - 202s 16ms/step - loss: 14.6022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1464ff550>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([input_X,input_masked_first,input_masked_second,input_masked_third],input_Y,epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = ['X_input_second','X_input_mask_2']\n",
    "\n",
    "input_predict = [input_X[:10],input_X[:10],input_X[:10],input_X[:10]]\n",
    "\n",
    "hidden_layer = []\n",
    "for layer_name in layerName:\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=model.get_layer(layer_name).output)\n",
    "    intermediate_output = intermediate_layer_model.predict(input_predict)\n",
    "    hidden_layer.append(intermediate_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentEval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
