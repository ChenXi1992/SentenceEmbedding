{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Warning: Normally its '  but in the text is ’\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def sentence2Vec(dataset,model,wordDim,maxLength):\n",
    "    sentenceVec = []\n",
    "    for sentence in dataset:\n",
    "        wordVec = []\n",
    "        for vocab in sentence:\n",
    "            if vocab == marked_token:\n",
    "                wordVec.append(np.ones(wordDim))\n",
    "            else:\n",
    "                try:\n",
    "                    vec =  model[vocab]\n",
    "                    wordVec.append(vec)\n",
    "                except :\n",
    "                    wordVec.append(np.zeros(wordDim))\n",
    "        sentenceVec.append(wordVec)\n",
    "\n",
    "    sentenceVec = keras.preprocessing.sequence.pad_sequences(sentenceVec, maxlen=maxLength, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    return sentenceVec \n",
    "\n",
    "\n",
    "# High frequency vocab dictionary \n",
    "def getHighFrequencyVocab(frequentVocabLimit,mergeList):\n",
    "    fdist1 = FreqDist(mergeList)\n",
    "    frequentVocab = fdist1.most_common(frequentVocabLimit)\n",
    "    frequentDic = {}\n",
    "    for vocab in frequentVocab:\n",
    "        frequentDic[vocab[0]] = True\n",
    "    return frequentDic\n",
    "\n",
    "\n",
    "def generateMaskedSentence(processedSentence,frequentDic):\n",
    "    for sentence in processedSentence:\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] not in frequentDic.keys():\n",
    "                sentence[i] = marked_token\n",
    "    return processedSentence\n",
    "\n",
    "def generateIndexForDataset(dataSize):\n",
    "    trainIndexList = []\n",
    "    data_index = []\n",
    "\n",
    "    for i in range(dataSize-1):\n",
    "        x = randint(0,dataSize-1)\n",
    "        y = randint(0,dataSize-1)\n",
    "        z = i+1 \n",
    "        while(x == y or x==i or x==z or y == i or y==z ):\n",
    "            x = randint(0,dataSize-1)\n",
    "            y = randint(0,dataSize-1)\n",
    "        index = ([[x,0],[y,0],[z,1]])\n",
    "        random.shuffle(index)\n",
    "        trainIndexList.append([i,index])   \n",
    "\n",
    "    for row in trainIndexList:\n",
    "        train_x = row[0]\n",
    "        y_index = row[1]\n",
    "        y_output = []\n",
    "        x_masked = []\n",
    "        for index in y_index:\n",
    "            y_output.append(index[1])\n",
    "            x_masked.append(index[0])\n",
    "        data_index.append([train_x,x_masked,y_output])\n",
    "\n",
    "    random.shuffle(data_index)\n",
    "\n",
    "    return data_index\n",
    "\n",
    "\n",
    "def loadEmbeddingModel(embeddingPath,word2VecFile,gloveFile,convertGloveToWordVec):\n",
    "    word2vec_output_file = embeddingPath + word2VecFile\n",
    "    glove_input_file = embeddingPath + gloveFile\n",
    "\n",
    "    if convertGloveToWordVec:\n",
    "        glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "    # load the Stanford GloVe model\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def loadFile(textPath,orgFile):\n",
    "    with open(textPath +  orgFile ,encoding=\"utf-8\") as fp:\n",
    "        lines = fp.readlines()\n",
    "    # Remove the empty line \n",
    "    lines = [e for e in lines if e not in {'\\n'}]\n",
    "    \n",
    "    return lines\n",
    "    \n",
    "def preprocessing(lines): \n",
    "    newLine = []\n",
    "    for line in lines:\n",
    "        # Decapitalize: Conver to lower case first. \n",
    "        line = line.lower()\n",
    "        # Delete url \n",
    "        line = re.sub(r\"http\\S+\", \"link\", line)\n",
    "        line = re.sub(r\"\\S+html\", \"link\", line)\n",
    "        line = re.sub(r\"\\S+.com$\", \"link\", line)\n",
    "        line = re.sub(r\"\\S+.jpg$\", \"photo\", line)\n",
    "        line = decontracted(line)\n",
    "\n",
    "        '''\n",
    "        ignore:  * { } \\  < > \n",
    "        Splite based on : ! . ,  &  # ' $ \n",
    "        line = re.findall(r\"[\\w']+|[().,:!?;'$&]\", line)\n",
    "        '''\n",
    "\n",
    "        line = re.findall(r\"[\\w']+|[().,:!?;'$&]\", line)\n",
    "\n",
    "        newLine.append(line)\n",
    "    \n",
    "    mergeList = []\n",
    "    for i in newLine:\n",
    "        mergeList += i   \n",
    "    return newLine, mergeList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file & Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle  \n",
    "from nltk import FreqDist\n",
    "import re\n",
    "import randomfa\n",
    "from random import randint\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np \n",
    "\n",
    "import keras\n",
    "# import tensorflow as tf\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Dense, Bidirectional,Input, Dropout, Flatten, concatenate, dot, GaussianDropout, Activation, GRU\n",
    "# from keras.regularizers import l1, l2\n",
    "import keras.backend as kb\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import Model\n",
    "\n",
    "embeddingPath = \"Embedding/\"\n",
    "convertGloveToWordVec = True  # The first time use Glove need to convert to Word2Vec format\n",
    "gloveFile = 'glove.840B.300d.txt'  \n",
    "word2VecFile =  'word2vec.6B.50d.txt'\n",
    "\n",
    "\n",
    "wordDim = 300 \n",
    "\n",
    "textPath = \"text/\"\n",
    "orgFile = \"all.txt\"\n",
    "processedFile = \"process.txt\"\n",
    "maskedFile = \"masked.txt\"\n",
    "\n",
    "frequentVocabLimit = 3500\n",
    "marked_token = \"MASKED_TOKEN\"\n",
    "trainingSize = 0\n",
    "maxLength = 50\n",
    "dataSize = 2000\n",
    "\n",
    "needProcessData = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if needProcessData:\n",
    "    # Load the data, remove the empty line\n",
    "    sentenceList = loadFile(textPath,orgFile)\n",
    "    # Remove unwanted feature, proprocessing, return wordList\n",
    "    processedLine, vocabList = preprocessing(sentenceList)\n",
    "\n",
    "    dataSize = len(processedLine)\n",
    "    # Get the high frequency vocabulary \n",
    "    frequenentVocabDic =  getHighFrequencyVocab(frequentVocabLimit,vocabList)\n",
    "\n",
    "    # Save processed data\n",
    "    with open(textPath +  processedFile, \"wb\") as internal_filename:\n",
    "        pickle.dump(processedLine, internal_filename)\n",
    "\n",
    "    # Generate the masked dataset\n",
    "    maskedLine =  generateMaskedSentence(processedLine,frequenentVocabDic)\n",
    "\n",
    "    # Save masked data\n",
    "    with open(textPath +  maskedFile, \"wb\") as internal_filename:\n",
    "        pickle.dump(maskedLine, internal_filename)\n",
    "    \n",
    "# Generate the index for the dataset \n",
    "dataIndex = generateIndexForDataset(dataSize)\n",
    "\n",
    "# Read the data \n",
    "with open(textPath + processedFile, \"rb\") as input_file:\n",
    "    processedLine = pickle.load(input_file)\n",
    "with open(textPath + maskedFile , \"rb\") as input_file:\n",
    "    maskedSentence = pickle.load(input_file)\n",
    "    \n",
    "embeddingMatrix = loadEmbeddingModel(embeddingPath,word2VecFile,gloveFile,convertGloveToWordVec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentenceVec =  sentence2Vec(processedLine,embeddingMatrix,wordDim,maxLength)\n",
    "maskedSentenceVec = sentence2Vec(processedLine,embeddingMatrix,wordDim,maxLength)\n",
    "\n",
    "embeddingMatrix = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate input_X, input_masked_X, input_Y\n",
    "input_X = []\n",
    "input_masked_first = []\n",
    "input_masked_second = []\n",
    "input_masked_third = []\n",
    "input_Y = []\n",
    "for index in dataIndex:\n",
    "    input_X.append(sentenceVec[index[0]])\n",
    "    input_Y.append(index[2])\n",
    "    input_masked_first.append(maskedSentenceVec[index[1][0]])\n",
    "    input_masked_second.append(maskedSentenceVec[index[1][1]])\n",
    "    input_masked_third.append(maskedSentenceVec[index[1][2]])\n",
    "    \n",
    "input_X = np.array(input_X)\n",
    "input_masked_first = np.array(input_masked_first)\n",
    "input_masked_second = np.array(input_masked_second)\n",
    "input_masked_third = np.array(input_masked_third)\n",
    "input_Y = np.array(input_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = int(0.99 * len(input_X))\n",
    "\n",
    "input_X_train = input_X[0:length]\n",
    "input_X_val = input_X[length:]\n",
    "input_X = 0 \n",
    "\n",
    "input_masked_first_train = input_masked_first[0:length]\n",
    "input_masked_first_val = input_masked_first[length:]\n",
    "input_masked_first = 0\n",
    "\n",
    "input_masked_second_train = input_masked_second[0:length]\n",
    "input_masked_second_val = input_masked_second[length:]\n",
    "input_masked_second = 0\n",
    "\n",
    "input_masked_third_train = input_masked_third[0:length]\n",
    "input_masked_third_val = input_masked_third[length:]\n",
    "input_masked_third = 0 \n",
    "\n",
    "input_Y_train = input_Y[0:length]\n",
    "input_Y_val = input_Y[length:]\n",
    "input_Y = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Modeling  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_neurons = [wordDim*2,wordDim*2]\n",
    "kernel_reg = 0.001\n",
    "batch_size = 512\n",
    "epochs = 5000\n",
    "\n",
    "merge_mode = 'concat'\n",
    "extractLayer = ['X_input_second','X_input_mask_2']\n",
    "checkpointPath = \"checkpoints/weights-epoch-{epoch:02d}-loss-{loss:.4f}-val_loss-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = \"checkpoints/weights-epoch-02-loss-11.88-val_loss-11.56.hdf5\"\n",
    "restoreCheckpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = np.zeros((10,maxLength,wordDim))\n",
    "model = Model.buildModel(input_X,kernel_reg,num_neurons,merge_mode)\n",
    "model.compile( loss='mse', optimizer='adam')\n",
    "if restoreCheckpoint:\n",
    "    model.load_weights(checkpoint)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint =  ModelCheckpoint(checkpointPath, monitor=('val_loss'), verbose=0, \n",
    "                              save_best_only=False, save_weights_only=False, mode='auto', period=50)\n",
    "\n",
    "model.fit([input_X_train,input_masked_first_train,input_masked_second_train,input_masked_third_train],input_Y_train,\n",
    "          validation_data=([input_X_val,input_masked_first_val,input_masked_second_val,input_masked_third_val],input_Y_val),\n",
    "          epochs=epochs,batch_size=batch_size,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output =  Model.extractHiddenState(extractLayer,model,np.zeros((10,maxLength,wordDim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output =  np.concatenate((output[0],output[1]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output =  Model.extractHiddenState(extractLayer,model,input_X)\n",
    "results1  =  np.concatenate((output[0],output[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the hidden layer for SentEval \n",
    "x =  np.zeros((10,maxLength,wordDim))\n",
    "model = buildModel(x,kernel_reg,num_neurons,merge_mode)\n",
    "model.load_weights(checkpoint)\n",
    "output = extractHiddenState(layerName=extractLayer,model=model, predict_input=input_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
